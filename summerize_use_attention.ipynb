{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPL7nUxLF7URBEUKIojaIm5"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpmhiSatWsvN","executionInfo":{"elapsed":2178,"status":"ok","timestamp":1610637347853,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"063e4fc3-2230-40ba-b19b-08be4981e28b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97Ws_KxVYteP","executionInfo":{"elapsed":3708,"status":"ok","timestamp":1610637349399,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"0e3aba76-a354-4eb3-f15c-9bded134f55c"},"source":["pip install nltk"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOaf0hDHPOOk","executionInfo":{"elapsed":3695,"status":"ok","timestamp":1610637349400,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"0f719251-42fc-4e4f-85e7-40981d75285a"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rXiGn3lzRl88","executionInfo":{"elapsed":40623,"status":"ok","timestamp":1610637386342,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"6ff41b05-956e-44aa-f95a-8b02ae4d59c8"},"source":["import numpy as np\n","import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from bs4 import BeautifulSoup\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import urllib.request\n","\n","np.random.seed(seed=0)\n","filename = '/content/drive/MyDrive/학습카드 자동 생성 프로젝트/test/Reviews.csv'\n","data = pd.read_csv(filename, nrows = 100000)\n","data = data[['Text', 'Summary']]\n","data.drop_duplicates(subset=['Text'], inplace=True)  #중복 제거\n","data.dropna(axis=0, inplace=True)  #Null 제거\n","\n","contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_sentence(sentence, remove_stopwords = True):\n","    sentence = sentence.lower() # 텍스트 소문자화\n","    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n","    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열  제거 Ex) my husband (and myself) for => my husband for\n","    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n","    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n","    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 제거. Ex) roland's -> roland\n","    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n","    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n","\n","    # 불용어 제거 (Text)\n","    if remove_stopwords:\n","        tokens = ' '.join(word for word in sentence.split() if not word in stop_words if len(word) > 1)\n","    # 불용어 미제거 (Summary)\n","    else:\n","        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n","    return tokens\n","\n","#전처리\n","clean_text = []\n","for s in data['Text'] :\n","    clean_text.append(preprocess_sentence(s))\n","clean_summary = []\n","for s in data['Summary'] :\n","    clean_summary.append(preprocess_sentence(s))\n","data['Text'] = clean_text\n","data['Summary'] = clean_summary\n","data.replace('', np.nan, inplace=True)\n","data.dropna(axis=0, inplace=True)\n","\n","#길이 맞추기\n","text_max_len = 50\n","summary_max_len = 8\n","data = data[data['Text'].apply(lambda x: len(x.split()) <= text_max_len)]\n","data = data[data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)]\n","\n","#시작, 종료 토큰\n","data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n","data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n","\n","#input, target\n","encoder_input = np.array(data['Text'])\n","decoder_input = np.array(data['decoder_input'])\n","decoder_target = np.array(data['decoder_target'])\n","\n","#순서 섞은 정수 시퀀스\n","indices = np.arange(encoder_input.shape[0])\n","np.random.shuffle(indices)\n","\n","#샘플 순서 섞어주기\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.amazon.com/gp/product/b007i7yygy/ref=cm_cr_rev_prod_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xVkyrhARrON","executionInfo":{"elapsed":46983,"status":"ok","timestamp":1610637392715,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"f631fc96-cb13-4a99-b6a3-39babe0874a3"},"source":["#훈련/테스트\n","n_of_val = int(len(encoder_input)*0.2)\n","\n","encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]\n","\n","#정수 인코딩\n","src_tokenizer = Tokenizer()\n","src_tokenizer.fit_on_texts(encoder_input_train)\n","\n","#등장 빈도 작은 단어가 차지하는 파이 확\n","# threshold = 7\n","# total_cnt = len(src_tokenizer.word_index) # 단어의 수\n","# rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n","# total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n","# rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n","#\n","# # 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n","# for key, value in src_tokenizer.word_counts.items():\n","#     total_freq = total_freq + value\n","#\n","#     # 단어의 등장 빈도수가 threshold보다 작으면\n","#     if(value < threshold):\n","#         rare_cnt = rare_cnt + 1\n","#         rare_freq = rare_freq + value\n","# print('단어 집합(vocabulary)의 크기 :',total_cnt)\n","# print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n","# print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n","# print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n","# print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n","\n","src_vocab=8000\n","src_tokenizer = Tokenizer(num_words = src_vocab)\n","src_tokenizer.fit_on_texts(encoder_input_train)\n","\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train)\n","encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n","\n","tar_vocab=2000\n","tar_tokenizer = Tokenizer(num_words=tar_vocab)\n","tar_tokenizer.fit_on_texts(decoder_input_train)\n","tar_tokenizer.fit_on_texts(decoder_target_train)\n","# 텍스트 시퀀스를 정수 시퀀스로 변환\n","decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train)\n","decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n","decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n","decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n","\n","#빈 샘플 제거\n","drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n","drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n","\n","encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n","decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n","decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n","\n","encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n","decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n","decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)\n","\n","#패딩\n","encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n","encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n","decoder_input_train = pad_sequences(decoder_input_train, maxlen = summary_max_len, padding='post')\n","decoder_target_train = pad_sequences(decoder_target_train, maxlen = summary_max_len, padding='post')\n","decoder_input_test = pad_sequences(decoder_input_test, maxlen = summary_max_len, padding='post')\n","decoder_target_test = pad_sequences(decoder_target_test, maxlen = summary_max_len, padding='post')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOLqxhhcbTIG","executionInfo":{"elapsed":47568,"status":"ok","timestamp":1610637393314,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"3af21628-7e27-4f9b-9025-e1431092e72c"},"source":["from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 128\n","hidden_size = 256\n","\n","# 인코더\n","encoder_inputs = Input(shape=(text_max_len,))\n","\n","# 인코더의 임베딩 층\n","enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n","\n","# 인코더의 LSTM 1\n","encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n","encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n","\n","# 인코더의 LSTM 2\n","encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n","encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n","\n","# 인코더의 LSTM 3\n","encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n","encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n","\n","# 디코더\n","decoder_inputs = Input(shape=(None,))\n","\n","# 디코더의 임베딩 층\n","dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","# 디코더의 LSTM\n","decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n","decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0IS17CCAbV3U"},"source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n","from attention import AttentionLayer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"PeOfsgzPbdS8","executionInfo":{"elapsed":261640,"status":"ok","timestamp":1610637786072,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"576f62ec-56e3-4035-92f7-cb7e5e780e5a"},"source":["# 어텐션 층(어텐션 함수)\n","attn_layer = AttentionLayer(name='attention_layer')\n","attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n","\n","# 어텐션의 결과와 디코더의 hidden state들을 연결\n","decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n","\n","# 디코더의 출력층\n","decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n","decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n","\n","# 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n","model.summary()\n","\n","model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n","history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n","          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n","          batch_size = 256, callbacks=[es], epochs = 3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 50)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 50, 128)      1024000     input_3[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_4 (LSTM)                   [(None, 50, 256), (N 394240      embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","lstm_5 (LSTM)                   [(None, 50, 256), (N 525312      lstm_4[0][0]                     \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, None, 128)    256000      input_4[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_6 (LSTM)                   [(None, 50, 256), (N 525312      lstm_5[0][0]                     \n","__________________________________________________________________________________________________\n","lstm_7 (LSTM)                   [(None, None, 256),  394240      embedding_3[0][0]                \n","                                                                 lstm_6[0][1]                     \n","                                                                 lstm_6[0][2]                     \n","__________________________________________________________________________________________________\n","attention_layer (AttentionLayer ((None, None, 256),  131328      lstm_6[0][0]                     \n","                                                                 lstm_7[0][0]                     \n","__________________________________________________________________________________________________\n","concat_layer (Concatenate)      (None, None, 512)    0           lstm_7[0][0]                     \n","                                                                 attention_layer[0][0]            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, None, 2000)   1026000     concat_layer[0][0]               \n","==================================================================================================\n","Total params: 4,276,432\n","Trainable params: 4,276,432\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/3\n","206/206 [==============================] - 1197s 6s/step - loss: 2.6261 - val_loss: 1.9665\n","Epoch 2/3\n","206/206 [==============================] - 1178s 6s/step - loss: 1.9723 - val_loss: 1.8578\n","Epoch 3/3\n","206/206 [==============================] - 1172s 6s/step - loss: 1.8477 - val_loss: 1.7841\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvlzcW6IYS3u","executionInfo":{"elapsed":28,"status":"ok","timestamp":1610637786073,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"ae3b58fe-e7cf-4365-ba15-b1a28c89a11d"},"source":["print(data['Text'][0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73},"id":"Ki_obF5msnLN","executionInfo":{"elapsed":20557,"status":"ok","timestamp":1610637806613,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"},"user_tz":-540},"outputId":"e8a597ec-94eb-4aed-ab74-640d15de1c2c"},"source":["from google.colab import files\n","myfile = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-012cb3dd-0fc8-49dd-b304-a067286e4b76\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-012cb3dd-0fc8-49dd-b304-a067286e4b76\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving src_naver_test_his_1.txt to src_naver_test_his_1.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LjZUVNeUfpHA"},"source":["#테스트\n","#숫자랑 특수문자는 일단 직접 한글로 바꿔둠\n","\n","with open('/content/src_naver_test_his_1.txt', 'r') as f :\n","  text = f.read() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-WDGbGtiHeO"},"source":["stopwords_kor = \"이번에 이거는 이렇게 그러니까 이게 바로 조금\"\n","stopwords_kor = stopwords_kor.split(' ')\n","\n","import re\n","\n","def preprocess_kor(input) :\n","  input = re.sub(\"\\n\", \" \", input)\n","  input = re.sub(\"[,.]\", \" \", input)\n","  tokens = ' '.join(word for word in input.split() if not word in stopwords_kor if len(word) > 1)\n","  return tokens\n","\n","pre_kor = preprocess_kor(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6wX0y-P7ZVY"},"source":["def romanize(raw, fromEnc = 'utf8', toEnc = 'utf8'):\n","  \"\"\"\n","    Takes a raw string of Korean, a 'from encoding' and a 'to encoding'.\n","    Returns a romanized string of the text, encoded as specified (default\n","    'from encoding' is None and default 'to encoding' is utf-8).\n","  \"\"\"\n","  newString = ''\n","  for i in range(len(raw)):\n","    index = gti(raw[i])\n","    \n","    # If the index is a single (non-syllabic) hangul letter\n","    if index in range(12593, 12687):\n","      index = index - 12593\n","      if singles[index] and len(newString) > 1 and newString[-1] != ' ':\n","        newString += '.'\n","      newString += '<' + singles[index] + '>'\n","   \n","    # If the index represents a hangul syllable\n","    elif index in range(44032, 55204):\n","      index = index - 44032\n","      initial = int(index / 588)\n","      vowel = int((index % 588) / 28)\n","      final = (index % 588) % 28\n","      if len(newString) > 0:\n","        if nonpachim[initial] == 'g' and newString[-1] == 'n':\n","          newString += '.'\n","        elif nonpachim[initial] == '' and newString[-2:len(newString)] == 'ng':\n","          newString += '.'\n","        elif (newString[-1] in moum or newString[-2:len(newString)] in moum) and nonpachim[initial] in pachim + nonpachim:\n","          newString += '.'\n","        elif nonpachim[initial] == '' and newString[-1] in pachim + nonpachim: \n","          newString += '.'\n","        elif nonpachim[initial] == 'h' and newString[-1] in ['t','k','p','c','n','l']: \n","          newString += '.'\n","        elif newString[-1] + nonpachim[initial] in pachim + nonpachim or (len(nonpachim[initial]) > 1 and newString[-1] + nonpachim[initial][0] in pachim + nonpachim):\n","          newString += '.' \n","      newString += nonpachim[int(initial)]\n","      newString += moum[int(vowel)]\n","      newString += pachim[int(final)]\n","    \n","    # Otherwise\n","    else:\n","      newString += chr(index).upper()\n","  return newString\n","\n","\n","def gti(char):\n","  \"\"\"\n","    Only accepts unicode characters\n","    Return index of characters\n","  \"\"\"\n","  return ord(char)\n","\n","\n","# Character lists\n","singles = ['k', 'kk', 'ks', 'n', 'nc', 'nh', 't', 'tt', 'l', 'lk', 'lm', 'lp', 'ls', 'lth', 'lph', 'lh', 'm', 'p', 'pp', 'ps', 's', 'ss', 'ng', 'c', 'cc', 'ch', 'kh', 'th', 'ph', 'h', 'a', 'ay', 'ya', 'yay', 'e', 'ey', 'ye', 'yey', 'o', 'wa', 'way', 'oy', 'yo', 'wu', 'we', 'wey', 'wi', 'yu', 'u', 'uy', 'i', 'NONE', 'NN', 'NT', 'NS', 'NZ', 'LKS', 'LT', 'LPS', 'LZ', 'LH', 'MP', 'MS', 'MZ', 'MNG', 'PK', 'PT', 'PSK', 'PST', 'PC', 'PTH', 'PNG', 'PPNG', 'SK', 'SL', 'ST', 'SP', 'SC', 'Z', 'NGNG', 'NG', 'NGS', 'NGZ', 'PHNG', 'HH', 'H', 'YOYA', 'YOYAY', 'YOI', 'YUE', 'YUEY', 'YUI', 'A', 'E']\n","moum = ['a', 'ay', 'ya', 'yay', 'e', 'ey', 'ye', 'yey', 'o', 'wa', 'way', 'oy', 'yo', 'wu', 'we', 'wey', 'wi', 'yu', 'u', 'uy', 'i']\n","pachim = ['', 'k', 'kk', 'ks', 'n', 'nc', 'nh', 't', 'l', 'lk', 'lm', 'lp', 'ls', 'lth', 'lph', 'lh', 'm', 'p', 'ps', 's', 'ss', 'ng', 'c', 'ch', 'kh', 'th', 'ph', 'h']\n","nonpachim = ['k', 'kk', 'n', 't', 'tt', 'l', 'm', 'p', 'pp', 's', 'ss', '', 'c', 'cc', 'ch', 'kh', 'th', 'ph', 'h']\n","letters = ['k', 'n', 't', 'l', 'm', 'p', 's', 'c', 'k', 'h', 'g', 'a', 'y', 'e', 'o', 'w', 'u', 'i']\n","vowelLetters = ['a', 'e', 'i', 'o', 'u', 'w', 'y']\n","consonantLetters = ['c', 'g', 'h', 'k', 'l', 'm', 'n', 'p', 's', 't', 'lt', 'lp']\n","singleLetters = ['k', 'kk', 'ks', 'ns', 'n', 'nc', 'nh', 't', 'tt', 'l', 'lk', 'lm', 'lp', 'ls', 'lth', 'lt', 'lph', 'lt', 'lh', 'm', 'p', 'pp', 'ps', 's', 'ss', 'ng', 'c', 'cc', 'ch', 'kh', 'th', 'ph', 'h', 'a', 'ay', 'ya', 'yay', 'e', 'ey', 'ye', 'yey', 'o', 'wa', 'way', 'oy', 'yo', 'wu', 'we', 'wey', 'wi', 'yu', 'u', 'uy', 'i', 'NONE', 'N', 'NO', 'NON', 'NN', 'NT', 'NS', 'NZ', 'LKS', 'LK', 'L', 'LT', 'LPS', 'LP', 'LZ', 'LH', 'MP', 'M', 'MS', 'MZ', 'MNG', 'MN', 'PK', 'P', 'PT', 'PSK', 'PS', 'PST', 'PC', 'PTH', 'PNG', 'PPNG', 'PP', 'PPN', 'SK', 'S', 'SL', 'ST', 'SP', 'SC', 'Z', 'NGNG', 'NGN', 'NG', 'NGS', 'NGZ', 'PHNG', 'PH', 'PHN', 'HH', 'H', 'YOYA', 'Y', 'YO', 'O', 'YOY', 'YOYAY', 'YOI', 'YUE', 'YU', 'YUEY', 'YUI', 'A', 'E']\n","\n","\n","# Character dictionaries\n","singlesDict = {'yey': 37, 'YUE': 89, 'YOYAY': 87, 'tt': 7, 'lm': 10, 'lk': 9, 'lh': 15, 'ls': 12, 'lp': 11, 'wey': 45, 'YUEY': 90, 'yo': 42, 'ya': 32, 'H': 85, 'LPS': 58, 'yu': 47, 'YUI': 91, 'h': 29, 'l': 8, 'p': 17, 't': 6, 'HH': 84, 'ey': 35, 'NGS': 81, 'n': 3, 'LKS': 56, 'NGZ': 82, 'NONE': 51, 'PT': 66, 'PTH': 70, 'PC': 69, 'PK': 65, 'MNG': 64, 'we': 44, 'wa': 39, 'PPNG': 72, 'wi': 46, 'wu': 43, 'PSK': 67, 'c': 23, 'k': 0, 'o': 38, 'PHNG': 83, 's': 20, 'MP': 61, 'MS': 62, 'YOYA': 86, 'lth': 13, 'PST': 68, 'MZ': 63, 'ch': 25, 'cc': 24, 'ps': 19, 'pp': 18, 'yay': 33, 'NN': 52, 'NG': 80, 'NZ': 55, 'way': 40, 'ph': 28, 'NS': 54, 'NT': 53, 'th': 27, 'Z': 78, 'uy': 49, 'SP': 76, 'ST': 75, 'SK': 73, 'ss': 21, 'SL': 74, 'SC': 77, 'ay': 31, 'NGNG': 79, 'nh': 5, 'nc': 4, 'LH': 60, 'ng': 22, 'LT': 57, 'ks': 2, 'LZ': 59, 'A': 92, 'E': 93, 'oy': 41, 'YOI': 88, 'ye': 36, 'kk': 1, 'a': 30, 'e': 34, 'i': 50, 'kh': 26, 'm': 16, 'u': 48, 'lph': 14, 'PNG': 71}\n","moumDict = {'a': 0, 'we': 14, 'uy': 19, 'yay': 3, 'oy': 11, 'wa': 9, 'ya': 2, 'yo': 12, 'ye': 6, 'o': 8, 'yey': 7, 'i': 20, 'wu': 13, 'ey': 5, 'wi': 16, 'way': 10, 'ay': 1, 'e': 4, 'wey': 15, 'yu': 17, 'u': 18}\n","pachimDict = {'': 0, 'nc': 5, 'ch': 23, 'nh': 6, 'ps': 18, 'p': 17, 'lm': 10, 'lk': 9, 'lh': 15, 'ng': 21, 'ls': 12, 'lp': 11, 'ph': 26, 'th': 25, 'c': 22, 'ss': 20, 'h': 27, 'k': 1, 'kh': 24, 'm': 16, 'l': 8, 'n': 4, 'ks': 3, 'kk': 2, 's': 19, 't': 7, 'lph': 14, 'lth': 13}\n","nonpachimDict = {'': 11, 'pp': 8, 'ch': 14, 'ss': 10, 'kk': 1, 'c': 12, 'k': 0, 'kh': 15, 'm': 6, 'l': 5, 'n': 2, 'p': 7, 's': 9, 't': 3, 'th': 16, 'h': 18, 'ph': 17, 'tt': 4, 'cc': 13}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2BVFrw3uyl-"},"source":["pre_eng = []\n","for w in pre_kor.split() :\n","  pre_eng.append(romanize(w))\n","\n","for w in range(len(pre_eng)) :\n","  pre_eng[w] = re.sub(\"[.]\", \"\", pre_eng[w])\n","plus_tokenizer = Tokenizer()\n","plus_tokenizer.fit_on_texts(pre_eng)\n","\n","plus_index = plus_tokenizer.index_word\n","\n","tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n","tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n","\n","length = len(tar_index_to_word)\n","\n","for i in range(len(plus_input)) :\n","  tar_index_to_word[length + i] = plus_index[i+1]\n","  tar_word_to_index[plus_index[i+1]] = length + i\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nu8vkLo0-SDh"},"source":["# 인코더 설계\n","encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n","\n","# 이전 시점의 상태들을 저장하는 텐서\n","decoder_state_input_h = Input(shape=(hidden_size,))\n","decoder_state_input_c = Input(shape=(hidden_size,))\n","\n","dec_emb2 = dec_emb_layer(decoder_inputs)\n","# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n","# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n","decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","# 어텐션 함수\n","decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n","attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n","decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n","\n","# 디코더의 출력층\n","decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n","\n","# 최종 디코더 모델\n","decoder_model = Model(\n","    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n","    [decoder_outputs2] + [state_h2, state_c2])\n","\n","def decode_sequence(input_seq):\n","    # 입력으로부터 인코더의 상태를 얻음\n","    e_out, e_h, e_c = encoder_model.predict(input_seq)\n","\n","     # <SOS>에 해당하는 토큰 생성\n","    target_seq = np.zeros((1,1))\n","    target_seq[0, 0] = tar_word_to_index['sostoken']\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n","\n","        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token = tar_index_to_word[sampled_token_index]\n","\n","        if(sampled_token!='eostoken'):\n","            decoded_sentence += ' '+sampled_token\n","\n","        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n","        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n","            stop_condition = True\n","\n","        # 길이가 1인 타겟 시퀀스를 업데이트\n","        target_seq = np.zeros((1,1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","        # 상태를 업데이트 합니다.\n","        e_h, e_c = h, c\n","\n","    return decoded_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPzTa0fATvk3","executionInfo":{"status":"ok","timestamp":1610649255676,"user_tz":-540,"elapsed":2283,"user":{"displayName":"안지오","photoUrl":"","userId":"03453238486187272136"}},"outputId":"416a567e-76b2-467f-a48e-773ce50fb219"},"source":["plus_input = []\n","num = len(pre_eng) // text_max_len\n","\n","for i in range(num+1) :\n","  temp = []\n","  for j in range(text_max_len) :\n","    if (i*text_max_len + j) == len(pre_eng) :\n","      break\n","    temp.append(pre_eng[i*text_max_len + j])\n","  plus_input.append(temp) \n","\n","plus_input = plus_tokenizer.texts_to_sequences(plus_input)\n","\n","for line in plus_input :\n","  for i in range(text_max_len - len(line)) :\n","    line.append(0)\n","#정수 시퀀스 데이터 준비 끝\n","\n","for i in range(num+1) :\n","  print(i)\n","  print(decode_sequence(np.array(plus_input[i]).reshape(1, text_max_len)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n"," great product\n","1\n"," great\n","2\n"," great product\n","3\n","\n","4\n"," great tea\n","5\n"," great\n"],"name":"stdout"}]}]}